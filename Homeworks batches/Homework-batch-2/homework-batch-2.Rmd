---
title: "Homework batch 2"
author: "Waheeb Algabri, William Berritt, Kossi Akplaka"
output:
  html_document:
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = TRUE)
```

```{r}
library(tidyverse)
library(fpp3)
library(readxl)
library(fpp2)
library(forecast)
library(AppliedPredictiveModeling)
library(caret)
library(pls)
library(elasticnet)
library(corrplot)

```

## Waheeb starts here

## Exersise 6.3

A chemical manufacturing process for a pharmaceutical product was discussed in Sect. 1.4. In this problem, the objective is to understand the relationship between biological measurements of the raw materials (predictors), measurements of the manufacturing process (predictors), and the response of product yield. Biological predictors cannot be changed but can be used to assess the quality of the raw material before processing. On the other hand, manufacturing process predictors can be changed in the manufacturing process. Improving product yield by 1 % will boost revenue by approximately one hundred thousand dollars per batch:

**(a)** - Start R and use these commands to load the data:

```{r}
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
```

The matrix processPredictors contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. yield contains the percent yield for each run.

**(b)** - A small percentage of cells in the predictor set contain missing values. Use an imputation function to fill in these missing values (e.g., see Sect. 3.8).

```{r}
sum(is.na(ChemicalManufacturingProcess))
```

```{r}
miss <- preProcess(ChemicalManufacturingProcess, method = "bagImpute")
Chemical <- predict(miss, ChemicalManufacturingProcess)

sum(is.na(Chemical))
```

In the Chemical Manufacturing Process dataset, there were 106 missing values. To impute these missing values, bagged trees were utilized, incorporating all other variables in the dataset for the imputation process.

**(c)** - Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter. What is the optimal value of the performance metric?

```{r echo=TRUE}
# filtering low frequencies
Chemical <- Chemical[, -nearZeroVar(Chemical)]
```

```{r}
set.seed(123)

# index for training
index <- createDataPartition(Chemical$Yield, p = .8, list = FALSE)

# train 
train_chem <- Chemical[index, ]

# test
test_chem <- Chemical[-index, ]
```

###  {.tabset}

#### Pls model

```{r}
# Pre-process the data (center and scale)
preProcValues <- preProcess(train_chem, method = c("center", "scale"))
train_chem <- predict(preProcValues, train_chem)
test_chem <- predict(preProcValues, test_chem)

# Define the trainControl object
ctrl <- trainControl(method = "cv", number = 5)

# Train a PLS model
set.seed(123)
plsTune <- train(
  Yield ~ ., 
  data = train_chem, 
  method = "pls", 
  tuneLength = 20, 
  trControl = ctrl, 
  preProc = c("center", "scale")
)

# Print the optimal model
print(plsTune)

# Plot the tuning results
plot(plsTune)

# Evaluate the model on the test set
predictions <- predict(plsTune, test_chem)
performance <- postResample(predictions, test_chem$Yield)
print(performance)
```

#### Elastic Net (enet) model

```{r}

# Train an Elastic Net model
set.seed(123)
enetTune <- train(
  Yield ~ ., 
  data = train_chem, 
  method = "enet", 
  tuneLength = 20, 
  trControl = ctrl, 
  preProc = c("center", "scale")
)

# Print the optimal model
print(enetTune)

# Plot the tuning results
plot(enetTune)

# Evaluate the model on the test set
predictions <- predict(enetTune, test_chem)
performance <- postResample(predictions, test_chem$Yield)
print(performance)

```

#### LARS (Least Angle Regression)

```{r}

set.seed(123)

larsTune <- train(Yield ~ ., Chemical , method = "lars", metric = "Rsquared",
                    tuneLength = 20, trControl = ctrl, preProc = c("center", "scale"))

plot(larsTune)

# Evaluate the model on the test set
predictions <- predict(larsTune, test_chem)
performance <- postResample(predictions, test_chem$Yield)
print(performance)

```

#### linear regression model

```{r}
lm_model <- lm(Yield ~ ., Chemical)

summary(lm_model)
```

The ordinary linear regression model has a Multiple R2 of 0.7817 and an Adjusted R2 of 0.6816.

#### Ridge regression

```{r}
set.seed(123)

## Define the candidate set of values
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))

ridgeTune <- train(Yield ~ ., Chemical , method = "ridge",
                     tuneGrid = ridgeGrid, trControl = ctrl, preProc = c("center", "scale"))

plot(ridgeTune)

# Evaluate the model on the test set
predictions <- predict(ridgeTune, test_chem)
performance <- postResample(predictions, test_chem$Yield)
print(performance)
```

------------------------------------------------------------------------

\###**(d)** - Predict the response for the test set. What is the value of the performance metric and how does this compare with the resampled performance metric on the training set?

Ordinary linear model had the highest $R^2$, but it comes with consequences. Therefore, the `Elastic net` method was chosen as it had the highest $R^2$.

The $R^2$ is 0.632168, which is higher than the training set.

```{r}
elastic_net_predict <- predict(enetTune, test_chem[ ,-1])

postResample(elastic_net_predict, test_chem[ ,1])
```

------------------------------------------------------------------------

### **(e)** - Which predictors are most important in the model you have trained? Do

either the biological or process predictors dominate the list?

```{r}
varImp(enetTune)
```

Both biological materials and manufacturing processes are crucial predictors in the analysis.Therefore, the ratio of process predictors to biological predictors in the top 20 list is approximately 14:6, which simplifies to 7:3

### **(f)** - Explore the relationships between each of the top predictors and the re- sponse. How could this information be helpful in improving yield in future runs of the manufacturing process?

```{r}
# Compute correlation matrix
cor_matrix <- cor(Chemical, use = "complete.obs")

# Extract correlations with yield
cor_with_yield <- cor_matrix[, "Yield"]

# Order predictors by their absolute correlation with Yield
top_predictors_cor <- names(sort(abs(cor_with_yield), decreasing = TRUE)[2:11])

# Order predictors by their absolute correlation with Yield
top_predictors_cor <- names(sort(abs(cor_with_yield), decreasing = TRUE)[2:11])


# Subset the correlation matrix to include only the top predictors and yield
cor_subset <- cor_matrix[c("Yield", top_predictors_cor), c("Yield", top_predictors_cor)]

# Plot the correlation matrix
corrplot(cor_subset, method = "color", type = "upper", tl.col = "black", tl.srt = 45, addCoef.col = "black")

```

The correlation plot indicates that `ManufacturingProcess32` has the highest positive correlation with Yield. Notably, three of the top ten variables show a negative correlation with Yield. This insight can be valuable for future manufacturing runs, highlighting the predictors that significantly impact yield. To maximize or improve yield, it is advisable to enhance the measurements of both the manufacturing process and the biological properties of the raw materials.

## Exercise 7.2

Friedman (1991) introduced several benchmark data sets create by simulation. One of these simulations used the following nonlinear equation to create data:

$$ y = 10 \sin(\pi x_1 x_2) + 20 (x_3 - 0.5)^2 + 10 x_4 + 5 x_5 + N(0, \sigma^2) $$

where the x values are random variables uniformly distributed between [0, 1] (there are also 5 other non-informative variables also created in the simulation). The package mlbench contains a function called mlbench.friedman1 that simulates these data:

```{r}
library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
featurePlot(trainingData$x, trainingData$y)
```

```{r}
## or other methods.

## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
```

Tune several models on these data.

###  {.tabset}

#### KNN

```{r}
knnModel <- train(x = trainingData$x, 
                  y = trainingData$y,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 10)

knnModel
```

#### NN

```{r}
# remove predictors to ensure maximum abs pairwise corr between predictors < 0.75
tooHigh <- findCorrelation(cor(trainingData$x), cutoff = .75)
# returns an empty variable

# create a tuning grid
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                        .size = c(1:10))


# 10-fold cross-validation to make reasonable estimates
ctrl <- trainControl(method = "cv", number = 10)

set.seed(200)

# tune
nnetTune <- train(trainingData$x, trainingData$y,
                  method = "nnet",
                  tuneGrid = nnetGrid,
                  trControl = ctrl,
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
                  maxit = 500)

nnetTune
```

#### MARS

```{r}
# create a tuning grid
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

set.seed(200)

# tune
marsTune <- train(trainingData$x, trainingData$y,
                  method = "earth",
                  tuneGrid = marsGrid,
                  trControl = trainControl(method = "cv"))

marsTune
```

#### SVM

```{r}
set.seed(200)

# tune
svmRTune <- train(trainingData$x, trainingData$y,
                  method = "svmRadial",
                  preProc = c("center", "scale"),
                  tuneLength = 14,
                  trControl = trainControl(method = "cv"))

svmRTune
```

### Which models appear to give the best performance? Does MARS select the informative predictors (those named X1â€“X5)?

```{r}
svmRPred <- predict(svmRTune, testData$x)

postResample(svmRPred, testData$y)
```

MARS appears to give the best performance since it has the lowest RMSE and MAE and highest $R^2$. The second best would be SVM radial.

```{r}
varImp(marsTune)
```

```{r}
plot(varImp(marsTune))
```

MARS successfully identifies the informative variables X1 through X5. However, X3 appears to be insignificant, with an importance value of approximately 0.

## Exercise 7.5.

Exercise 6.3 describes data for a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several nonlinear regression models.

```{r}
data(ChemicalManufacturingProcess)

# imputation
miss <- preProcess(ChemicalManufacturingProcess, method = "bagImpute")
Chemical <- predict(miss, ChemicalManufacturingProcess)

# filtering low frequencies
Chemical <- Chemical[, -nearZeroVar(Chemical)]

set.seed(200)

# index for training
index <- createDataPartition(Chemical$Yield, p = .8, list = FALSE)

# train 
train_x <- Chemical[index, -1]
train_y <- Chemical[index, 1]

# test
test_x <- Chemical[-index, -1]
test_y <- Chemical[-index, 1]
```

**a** - Which nonlinear regression model gives the optimal resampling and test set performance? 

### {.tabset}

#### KNN

```{r}
knnModel <- train(train_x, train_y,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 10)

knnModel
```

#### NN
```{r}
# remove predictors to ensure maximum abs pairwise corr between predictors < 0.75
tooHigh <- findCorrelation(cor(train_x), cutoff = .75)

# removing 21 variables
train_x_nnet <- train_x[, -tooHigh]
test_x_nnet <- test_x[, -tooHigh]

# create a tuning grid
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                        .size = c(1:10))

# 10-fold cross-validation to make reasonable estimates
ctrl <- trainControl(method = "cv", number = 10)

set.seed(200)

# tune
nnetTune <- train(train_x_nnet, train_y,
                  method = "nnet",
                  tuneGrid = nnetGrid,
                  trControl = ctrl,
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 10 * (ncol(train_x_nnet) + 1) + 10 + 1,
                  maxit = 500)

nnetTune
```

#### MARS
```{r}
# create a tuning grid
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

set.seed(200)

# tune
marsTune <- train(train_x, train_y,
                  method = "earth",
                  tuneGrid = marsGrid,
                  trControl = trainControl(method = "cv"))

marsTune
```


#### SVM 
```{r}
set.seed(200)

# tune
svmRTune <- train(train_x, train_y,
                  method = "svmRadial",
                  preProc = c("center", "scale"),
                  tuneLength = 14,
                  trControl = trainControl(method = "cv"))

svmRTune
```

## let's rebind them togather

```{r}
# Combine results
results <- rbind(knn = postResample(predict(knnModel, newdata = test_x), test_y),
                 nn = postResample(predict(nnetTune, newdata = test_x_nnet), test_y),
                 mars = postResample(predict(marsTune, newdata = test_x), test_y),
                 svmR = postResample(predict(svmRTune, newdata = test_x), test_y))

results
```

Based on these metrics, svmR appears to be the optimal choice among the models evaluated in terms of predictive performance for the given chemical manufacturing process dataset.


**(b)** -  Which predictors are most important in the optimal nonlinear regres- sion model? Do either the biological or process variables dominate the list? How do the top ten important predictors compare to the top ten predictors from the optimal linear model?

```{r}
varImp(svmRTune)
```

```{r}
plot(varImp(svmRTune), top = 20) 
```

The process variables dominate the list with a ratio of 16:4.

**(c)** -  Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model. Do these plots reveal intuition about the biological or process predictors and their relationship with yield?


```{r}
# Compute correlation matrix
cor_matrix <- cor(Chemical, use = "complete.obs")

# Extract top 10 important predictors from SVM model
top10 <- varImp(svmRTune)$importance %>%
  arrange(-Overall) %>%
  head(10)

# Extract names of top 10 predictors
top10_names <- row.names(top10)

# Subset Chemical data to include only Yield and top 10 predictors
Chemical_subset <- Chemical[, c("Yield", top10_names)]

# Compute correlation matrix for top 10 predictors and Yield
cor_top10 <- cor(Chemical_subset, use = "complete.obs")

# Plot correlation matrix for top 10 predictors and Yield
corrplot(cor_top10, method = "color", type = "upper", tl.col = "black", tl.srt = 45, addCoef.col = "black")

```


```{r}
train_x %>%
  select(row.names(top10)) %>%
  featurePlot(., train_y)
```

According to the correlation plot, `ManufacturingProcess32` shows the highest positive correlation with Yield. Additionally, two of the top ten variables exhibit a negative correlation with Yield.

It appears that `the biological predictors` generally exhibit a positive relationship with yield, whereas the relationships of `manufacturing processes` with yield vary. For example, `ManufacturingProcess31` tends to cluster around a specific value, whereas `ManufacturingProcess36` shows distinct levels.














