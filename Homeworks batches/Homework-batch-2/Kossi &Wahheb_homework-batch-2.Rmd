---
title: "Homework batch 2"
author: "Waheeb Algabri, William Berritt, Kossi Akplaka"
output:
  html_document:
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = TRUE)
```

```{r}
library(tidyverse)
library(fpp3)
library(readxl)
library(fpp2)
library(forecast)
library(AppliedPredictiveModeling)
library(caret)
library(pls)
library(elasticnet)
library(corrplot)

```


## Exersise 6.3

A chemical manufacturing process for a pharmaceutical product was discussed in Sect. 1.4. In this problem, the objective is to understand the relationship between biological measurements of the raw materials (predictors), measurements of the manufacturing process (predictors), and the response of product yield. Biological predictors cannot be changed but can be used to assess the quality of the raw material before processing. On the other hand, manufacturing process predictors can be changed in the manufacturing process. Improving product yield by 1 % will boost revenue by approximately one hundred thousand dollars per batch:

**(a)** - Start R and use these commands to load the data:

```{r}
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
```

The matrix processPredictors contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. yield contains the percent yield for each run.

**(b)** - A small percentage of cells in the predictor set contain missing values. Use an imputation function to fill in these missing values (e.g., see Sect. 3.8).

```{r}
sum(is.na(ChemicalManufacturingProcess))
```

```{r}
miss <- preProcess(ChemicalManufacturingProcess, method = "bagImpute")
Chemical <- predict(miss, ChemicalManufacturingProcess)

sum(is.na(Chemical))
```

In the Chemical Manufacturing Process dataset, there were 106 missing values. To impute these missing values, bagged trees were utilized, incorporating all other variables in the dataset for the imputation process.

**(c)** - Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter. What is the optimal value of the performance metric?

```{r echo=TRUE}
# filtering low frequencies
Chemical <- Chemical[, -nearZeroVar(Chemical)]
```

```{r}
set.seed(123)

# index for training
index <- createDataPartition(Chemical$Yield, p = .8, list = FALSE)

# train 
train_chem <- Chemical[index, ]

# test
test_chem <- Chemical[-index, ]
```

###  {.tabset}

#### Pls model

```{r}
# Pre-process the data (center and scale)
preProcValues <- preProcess(train_chem, method = c("center", "scale"))
train_chem <- predict(preProcValues, train_chem)
test_chem <- predict(preProcValues, test_chem)

# Define the trainControl object
ctrl <- trainControl(method = "cv", number = 5)

# Train a PLS model
set.seed(123)
plsTune <- train(
  Yield ~ ., 
  data = train_chem, 
  method = "pls", 
  tuneLength = 20, 
  trControl = ctrl, 
  preProc = c("center", "scale")
)

# Print the optimal model
print(plsTune)

# Plot the tuning results
plot(plsTune)

# Evaluate the model on the test set
predictions <- predict(plsTune, test_chem)
performance <- postResample(predictions, test_chem$Yield)
print(performance)
```

#### Elastic Net (enet) model

```{r}

# Train an Elastic Net model
set.seed(123)
enetTune <- train(
  Yield ~ ., 
  data = train_chem, 
  method = "enet", 
  tuneLength = 20, 
  trControl = ctrl, 
  preProc = c("center", "scale")
)

# Print the optimal model
print(enetTune)

# Plot the tuning results
plot(enetTune)

# Evaluate the model on the test set
predictions <- predict(enetTune, test_chem)
performance <- postResample(predictions, test_chem$Yield)
print(performance)

```

#### LARS (Least Angle Regression)

```{r}

set.seed(123)

larsTune <- train(Yield ~ ., Chemical , method = "lars", metric = "Rsquared",
                    tuneLength = 20, trControl = ctrl, preProc = c("center", "scale"))

plot(larsTune)

# Evaluate the model on the test set
predictions <- predict(larsTune, test_chem)
performance <- postResample(predictions, test_chem$Yield)
print(performance)

```

#### linear regression model

```{r}
lm_model <- lm(Yield ~ ., Chemical)

summary(lm_model)
```

The ordinary linear regression model has a Multiple R2 of 0.7817 and an Adjusted R2 of 0.6816.

#### Ridge regression

```{r}
set.seed(123)

## Define the candidate set of values
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))

ridgeTune <- train(Yield ~ ., Chemical , method = "ridge",
                     tuneGrid = ridgeGrid, trControl = ctrl, preProc = c("center", "scale"))

plot(ridgeTune)

# Evaluate the model on the test set
predictions <- predict(ridgeTune, test_chem)
performance <- postResample(predictions, test_chem$Yield)
print(performance)
```

------------------------------------------------------------------------

\###**(d)** - Predict the response for the test set. What is the value of the performance metric and how does this compare with the resampled performance metric on the training set?

Ordinary linear model had the highest $R^2$, but it comes with consequences. Therefore, the `Elastic net` method was chosen as it had the highest $R^2$.

The $R^2$ is 0.632168, which is higher than the training set.

```{r}
elastic_net_predict <- predict(enetTune, test_chem[ ,-1])

postResample(elastic_net_predict, test_chem[ ,1])
```

------------------------------------------------------------------------

### **(e)** - Which predictors are most important in the model you have trained? Do

either the biological or process predictors dominate the list?

```{r}
varImp(enetTune)
```

Both biological materials and manufacturing processes are crucial predictors in the analysis.Therefore, the ratio of process predictors to biological predictors in the top 20 list is approximately 14:6, which simplifies to 7:3

### **(f)** - Explore the relationships between each of the top predictors and the re- sponse. How could this information be helpful in improving yield in future runs of the manufacturing process?

```{r}
# Compute correlation matrix
cor_matrix <- cor(Chemical, use = "complete.obs")

# Extract correlations with yield
cor_with_yield <- cor_matrix[, "Yield"]

# Order predictors by their absolute correlation with Yield
top_predictors_cor <- names(sort(abs(cor_with_yield), decreasing = TRUE)[2:11])

# Order predictors by their absolute correlation with Yield
top_predictors_cor <- names(sort(abs(cor_with_yield), decreasing = TRUE)[2:11])


# Subset the correlation matrix to include only the top predictors and yield
cor_subset <- cor_matrix[c("Yield", top_predictors_cor), c("Yield", top_predictors_cor)]

# Plot the correlation matrix
corrplot(cor_subset, method = "color", type = "upper", tl.col = "black", tl.srt = 45, addCoef.col = "black")

```

The correlation plot indicates that `ManufacturingProcess32` has the highest positive correlation with Yield. Notably, three of the top ten variables show a negative correlation with Yield. This insight can be valuable for future manufacturing runs, highlighting the predictors that significantly impact yield. To maximize or improve yield, it is advisable to enhance the measurements of both the manufacturing process and the biological properties of the raw materials.

## Exercise 7.2

Friedman (1991) introduced several benchmark data sets create by simulation. One of these simulations used the following nonlinear equation to create data:

$$ y = 10 \sin(\pi x_1 x_2) + 20 (x_3 - 0.5)^2 + 10 x_4 + 5 x_5 + N(0, \sigma^2) $$

where the x values are random variables uniformly distributed between [0, 1] (there are also 5 other non-informative variables also created in the simulation). The package mlbench contains a function called mlbench.friedman1 that simulates these data:

```{r}
library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
featurePlot(trainingData$x, trainingData$y)
```

```{r}
## or other methods.

## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
```

Tune several models on these data.

###  {.tabset}

#### KNN

```{r}
knnModel <- train(x = trainingData$x, 
                  y = trainingData$y,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 10)

knnModel
```

#### NN

```{r}
# remove predictors to ensure maximum abs pairwise corr between predictors < 0.75
tooHigh <- findCorrelation(cor(trainingData$x), cutoff = .75)
# returns an empty variable

# create a tuning grid
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                        .size = c(1:10))


# 10-fold cross-validation to make reasonable estimates
ctrl <- trainControl(method = "cv", number = 10)

set.seed(200)

# tune
nnetTune <- train(trainingData$x, trainingData$y,
                  method = "nnet",
                  tuneGrid = nnetGrid,
                  trControl = ctrl,
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
                  maxit = 500)

nnetTune
```

#### MARS

```{r}
# create a tuning grid
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

set.seed(200)

# tune
marsTune <- train(trainingData$x, trainingData$y,
                  method = "earth",
                  tuneGrid = marsGrid,
                  trControl = trainControl(method = "cv"))

marsTune
```

#### SVM

```{r}
set.seed(200)

# tune
svmRTune <- train(trainingData$x, trainingData$y,
                  method = "svmRadial",
                  preProc = c("center", "scale"),
                  tuneLength = 14,
                  trControl = trainControl(method = "cv"))

svmRTune
```

### Which models appear to give the best performance? Does MARS select the informative predictors (those named X1–X5)?

```{r}
svmRPred <- predict(svmRTune, testData$x)

postResample(svmRPred, testData$y)
```

MARS appears to give the best performance since it has the lowest RMSE and MAE and highest $R^2$. The second best would be SVM radial.

```{r}
varImp(marsTune)
```

```{r}
plot(varImp(marsTune))
```

MARS successfully identifies the informative variables X1 through X5. However, X3 appears to be insignificant, with an importance value of approximately 0.

## Exercise 7.5.

Exercise 6.3 describes data for a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several nonlinear regression models.

```{r}
data(ChemicalManufacturingProcess)

# imputation
miss <- preProcess(ChemicalManufacturingProcess, method = "bagImpute")
Chemical <- predict(miss, ChemicalManufacturingProcess)

# filtering low frequencies
Chemical <- Chemical[, -nearZeroVar(Chemical)]

set.seed(200)

# index for training
index <- createDataPartition(Chemical$Yield, p = .8, list = FALSE)

# train 
train_x <- Chemical[index, -1]
train_y <- Chemical[index, 1]

# test
test_x <- Chemical[-index, -1]
test_y <- Chemical[-index, 1]
```

**a** - Which nonlinear regression model gives the optimal resampling and test set performance? 

### {.tabset}

#### KNN

```{r}
knnModel <- train(train_x, train_y,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 10)

knnModel
```

#### NN
```{r}
# remove predictors to ensure maximum abs pairwise corr between predictors < 0.75
tooHigh <- findCorrelation(cor(train_x), cutoff = .75)

# removing 21 variables
train_x_nnet <- train_x[, -tooHigh]
test_x_nnet <- test_x[, -tooHigh]

# create a tuning grid
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                        .size = c(1:10))

# 10-fold cross-validation to make reasonable estimates
ctrl <- trainControl(method = "cv", number = 10)

set.seed(200)

# tune
nnetTune <- train(train_x_nnet, train_y,
                  method = "nnet",
                  tuneGrid = nnetGrid,
                  trControl = ctrl,
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 10 * (ncol(train_x_nnet) + 1) + 10 + 1,
                  maxit = 500)

nnetTune
```

#### MARS
```{r}
# create a tuning grid
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

set.seed(200)

# tune
marsTune <- train(train_x, train_y,
                  method = "earth",
                  tuneGrid = marsGrid,
                  trControl = trainControl(method = "cv"))

marsTune
```


#### SVM 
```{r}
set.seed(200)

# tune
svmRTune <- train(train_x, train_y,
                  method = "svmRadial",
                  preProc = c("center", "scale"),
                  tuneLength = 14,
                  trControl = trainControl(method = "cv"))

svmRTune
```

## let's rebind them togather

```{r}
# Combine results
results <- rbind(knn = postResample(predict(knnModel, newdata = test_x), test_y),
                 nn = postResample(predict(nnetTune, newdata = test_x_nnet), test_y),
                 mars = postResample(predict(marsTune, newdata = test_x), test_y),
                 svmR = postResample(predict(svmRTune, newdata = test_x), test_y))

results
```

Based on these metrics, svmR appears to be the optimal choice among the models evaluated in terms of predictive performance for the given chemical manufacturing process dataset.


**(b)** -  Which predictors are most important in the optimal nonlinear regres- sion model? Do either the biological or process variables dominate the list? How do the top ten important predictors compare to the top ten predictors from the optimal linear model?

```{r}
varImp(svmRTune)
```

```{r}
plot(varImp(svmRTune), top = 20) 
```

The process variables dominate the list with a ratio of 16:4.

**(c)** -  Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model. Do these plots reveal intuition about the biological or process predictors and their relationship with yield?


```{r}
# Compute correlation matrix
cor_matrix <- cor(Chemical, use = "complete.obs")

# Extract top 10 important predictors from SVM model
top10 <- varImp(svmRTune)$importance %>%
  arrange(-Overall) %>%
  head(10)

# Extract names of top 10 predictors
top10_names <- row.names(top10)

# Subset Chemical data to include only Yield and top 10 predictors
Chemical_subset <- Chemical[, c("Yield", top10_names)]

# Compute correlation matrix for top 10 predictors and Yield
cor_top10 <- cor(Chemical_subset, use = "complete.obs")

# Plot correlation matrix for top 10 predictors and Yield
corrplot(cor_top10, method = "color", type = "upper", tl.col = "black", tl.srt = 45, addCoef.col = "black")

```


```{r}
train_x %>%
  select(row.names(top10)) %>%
  featurePlot(., train_y)
```

According to the correlation plot, `ManufacturingProcess32` shows the highest positive correlation with Yield. Additionally, two of the top ten variables exhibit a negative correlation with Yield.

It appears that `the biological predictors` generally exhibit a positive relationship with yield, whereas the relationships of `manufacturing processes` with yield vary. For example, `ManufacturingProcess31` tends to cluster around a specific value, whereas `ManufacturingProcess36` shows distinct levels.


# Excercice 8.7

Refer to Exercises 6.3 and 7.5 which describe a chemical manufacturing
process. Use the same data imputation, data splitting, and pre-processing
steps as before and train several tree-based models:

```{r}
data(ChemicalManufacturingProcess)

# imputation
miss <- preProcess(ChemicalManufacturingProcess, method = "bagImpute")
Chemical <- predict(miss, ChemicalManufacturingProcess)

# filtering low frequencies
Chemical <- Chemical[, -nearZeroVar(Chemical)]

set.seed(200)

# index for training
index <- createDataPartition(Chemical$Yield, p = .8, list = FALSE)

# train 
train_x <- Chemical[index, -1]
train_y <- Chemical[index, 1]

# test
test_x <- Chemical[-index, -1]
test_y <- Chemical[-index, 1]
```


**(a)** Which tree-based regression model gives the optimal resampling and test set performance?

### Random Forest

```{r}
library(randomForest)

# Train Random Forest model
rf_model <- randomForest(train_y ~ ., train_x, importance = TRUE)

# Predict on test set
rf_predictions <- predict(rf_model, test_x)

# Evaluate performance
rf_performance <- postResample(rf_predictions, test_y)
print(paste("Random Forest RMSE:", sqrt(mean((rf_predictions - test_y)^2))))
print(paste("Random Forest R-squared:", cor(rf_predictions, test_y)^2))

```

### Gradient Boosting Machine

```{r}
library(gbm)

# Train GBM model
gbm_model <- gbm(train_y ~ ., train_x, distribution = "gaussian")

# Predict on test set
gbm_predictions <- predict(gbm_model, test_x, n.trees = 100)

# Evaluate performance
gbm_performance <- postResample(gbm_predictions, test_y)
print(paste("GBM RMSE:", sqrt(mean((gbm_predictions - test_y)^2))))
print(paste("GBM R-squared:", cor(gbm_predictions, test_y)^2))

```

### Decision Tree

```{r}
library(rpart)

# Train Decision Tree model
tree_model <- rpart(train_y ~ .,train_x)

# Predict on test set
tree_predictions <- predict(tree_model, test_x)

# Evaluate performance
tree_performance <- postResample(tree_predictions, test_y)
print(paste("Decision Tree RMSE:", sqrt(mean((tree_predictions - test_y)^2))))
print(paste("Decision Tree R-squared:", cor(tree_predictions, test_y)^2))
```

Random Forest shows the highest R-squared, suggesting it is be the optimal model for predicting yield in the chemical manufacturing process

**(b)** Which predictors are most important in the optimal tree-based regression model? Do either the biological or process variables dominate the list? How do the top 10 important predictors compare to the top 10 predictors from the optimal linear and nonlinear models?

### Importance of the predictors

```{r}
# Extract variable importance
var_importance <- importance(rf_model)

# Sort variables by importance
var_importance_sorted <- var_importance[order(var_importance[, "IncNodePurity"], decreasing = TRUE), ]

# Print the top 10 most important predictors
print(head(var_importance_sorted, 10))
```

The manufacturing process variables are still dominating the list like the other predictors from the optimal linear and nonlinear models.

**(c)** Plot the optimal single tree with the distribution of yield in the terminal nodes. Does this view of the data provide additional knowledge about the biological or process predictors and their relationship with yield?

### Plot the optimal tree

```{r message=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot)
#Single tree
rpart_tree <- rpart(Yield ~., Chemical)

#produce tree plot
rpart.plot(rpart_tree)

```


The decision tree plot reveals that manufacturing process variables play a dominant role in predicting yield in the chemical manufacturing process. The root node split on ManufacturingProcess32 indicates its critical importance, followed by other significant process variables. These variables appear in higher-level splits, emphasizing their substantial impact on yield outcomes. 


# Market Basket Analysis

Question:

I am assigning one simple problem on market basket analysis / recommender systems.  

Imagine 10000 receipts sitting on your table. Each receipt represents a transaction with items that were purchased. The receipt is a representation of stuff that went into a customer’s basket – and therefore ‘Market Basket Analysis’.

That is exactly what the Groceries Data Set contains: a collection of receipts with each line representing 1 receipt and the items purchased. Each line is called a transaction and each column in a row represents an item.

The dataset (comma separated file) is attached to this post.

You assignment is to use R to mine the data for association rules.  Provide information on  all relevant statistics like support, confidence, lift and others.  Also, Provide your top 10 rules by lift with the associated metrics. 

Answer:

## Load necessary libraries

We'll need the arules package for association rule mining.

```{r load-packages, message=FALSE}
library(arules)
```

## Load the dataset

I've uploaded the data into GitHub for reproducibility. We'll load it into R using read.csv.

```{r}
# Load the dataset
groceries <- read.transactions("https://raw.githubusercontent.com/Kossi-Akplaka/Data624/main/GroceryDataSet.csv", format = "basket", sep = ",")
groceries
```

## Data exploration

Let's explore the top 10 items in the dataset

```{r}
itemFrequencyPlot(groceries,topN=10, type = "absolute")
```


##  Mine Association Rules

Now, we'll use the apriori function to mine association rules. We'll specify the parameters such as, confidence, and lift.

```{r message=FALSE, warning=FALSE}
rules <- apriori(groceries, parameter = list(supp = 0.001, conf = 0.8))

# Sort rules by confidence (optional)
rules <- sort(rules, by = "confidence", decreasing = TRUE)
```
 Here, we use the Apriori algorithm to mine association rules from the groceries dataset, setting a minimum support threshold of 0.001 (0.1% of transactions) and a minimum confidence threshold of 0.8 (80%)
 
## Display the top 10 rules by lift with the associated metrics

```{r}
# Display top rules
inspect(rules[1:10])
```

The output shows the top 10 association rules mined from the dataset, sorted by confidence. Each rule consists of a left-hand side (lhs) and a right-hand side (rhs), with metrics such as support, confidence, coverage, lift, and count. 

For example, the first rule indicates that if a customer buys both rice and sugar, they are 100% likely to also buy whole milk (confidence = 1), with this rule applying to 0.12% of transactions (support = 0.0012). The lift value of 3.91 indicates that buying rice and sugar increases the likelihood of buying whole milk by approximately 3.91 times compared to random chance. The count column shows the absolute number of transactions that support this rule.

## Summary of the rules

```{r}
# Summary of rules
summary(rules)
```












